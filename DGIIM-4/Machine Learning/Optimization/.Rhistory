library(rpart)
library(rattle)
model= rpart(formula = income ~ ., data = training_set)
fancyRpartPlot(model)
y_pred = predict(model, newdata = training_set, type = 'class')
table(y_pred,training_set$income)
error=1-acc(y_pred,training_set$income)
#error on the training set for the optimal model
error
model.y_pred = predict(model, newdata = test_set, type = 'class')
table(model.y_pred,test_set$income)
error=1-acc(model.y_pred,test_set$income)
#error on the test set for the optimal model
error
#error on the test set for the optimal model
error # 0.1773
#ROC curve for the optimal model
roc.function(model.y_pred, test_set$income)
#CROSS VALIDATION
library(caret)
#CROSS VALIDATION
library(caret)
control = trainControl(method="cv", number=10)
model.caret = train(income ~ .,data=training_set, method="rpart", preProcess="scale", trControl=control)
plot(model.caret)
model.caret
model.caret$resample$Accuracy
sd(model.caret$resample$Accuracy)
cv.y_pred = predict(model.caret, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(model.y_pred,test_set$income)
#error on the test set for the optimal model
error
error=1-acc(cv.y_pred,test_set$income)
#error on the test set for the optimal model
error
#error on the test set for the optimal model
error # 0.1795
roc.function(cv.y_pred,test_set$income)
control = trainControl(method="repeatedcv", number=10,repeats=3)
model.caret2 = train(income ~ .,data=training_set, method="rpart", preProcess="scale", trControl=control)
plot(model.caret2)
model.caret2
model.caret2$resample$Accuracy
sd(model.caret2$resample$Accuracy)
cv.y_pred.2 = predict(model.caret2, newdata = test_set, type = 'raw')
table(cv.y_pred.2,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
#error on the test set for the optimal model
error
error=1-acc(cv.y_pred.2,test_set$income)
#error on the test set for the optimal model
error
# HYPERPARAMETERS
control = trainControl(method="cv", number=10)
parameter_grid = expand.grid(cp=c(0.01, 0.1, 0.2, 0.0005))
model.caret4 = train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control, tuneGrid = parameter_grid)
plot(model.caret4)
model.caret4
cv.y_pred = predict(model.caret4, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error
error # 0.1705
roc.function(cv.y_pred,test_set$income)
roc.function(cv.y_pred,test_set$income) # 0.73
model.randomSearch<- train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control,tuneLength = 15)
model.randomSearch
plot(model.randomSearch)
cv.y_pred = predict(model.randomSearch, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error
error # 0.1707
roc.function(cv.y_pred,test_set$income)
# BOOTSTRAP
control = trainControl(method = "boot632", number = 3)
model.boot <- train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control)
model.boot
plot(model.boot)
cv.y_pred = predict(model.boot, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(model.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error
roc.function(cv.y_pred,test_set$income)
acc <- function(y.true, y.pred) { sum(y.pred==y.true)/length(y.true) }
library(ROCR)
roc.function<-function(y_pred,testY){
pred <- prediction(as.numeric(y_pred), as.numeric(testY))
perf.auc <- performance(pred, measure = "auc")
auc<-round(unlist(perf.auc@y.values),2)
perf <- performance(pred,"tpr","fpr")
plot(perf,main=paste("ROC and AUC=",auc),colorize=TRUE, lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2) #diagonal
}
dataset<-read.csv("income.csv", stringsAsFactors = T)
# install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$income, SplitRatio = 0.7)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# install.packages("rpart")
library(rpart)
library(rattle)
model= rpart(formula = income ~ ., data = training_set)
fancyRpartPlot(model)
y_pred = predict(model, newdata = training_set, type = 'class')
table(y_pred,training_set$income)
error=1-acc(y_pred,training_set$income)
#error on the training set for the optimal model
error # 0.1644
roc.function(y_pred,test_set$income)
y_pred = predict(model, newdata = training_set, type = 'class')
table(y_pred,training_set$income)
error=1-acc(y_pred,training_set$income)
#error on the training set for the optimal model
error # 0.1644
roc.function(y_pred,test_set$income)
model.y_pred = predict(model, newdata = test_set, type = 'class')
table(model.y_pred,test_set$income)
error=1-acc(model.y_pred,test_set$income)
#error on the test set for the optimal model
error # 0.1773
#ROC curve for the optimal model
roc.function(model.y_pred, test_set$income) # 0.73
#CROSS VALIDATION
library(caret)
control = trainControl(method="cv", number=10)
model.caret = train(income ~ .,data=training_set, method="rpart", preProcess="scale", trControl=control)
plot(model.caret)
model.caret
model.caret$resample$Accuracy
sd(model.caret$resample$Accuracy)
cv.y_pred = predict(model.caret, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
#error on the test set for the optimal model
error # 0.1795
roc.function(cv.y_pred,test_set$income)
control = trainControl(method="repeatedcv", number=10,repeats=3)
model.caret2 = train(income ~ .,data=training_set, method="rpart", preProcess="scale", trControl=control)
plot(model.caret2)
model.caret2
model.caret2$resample$Accuracy
sd(model.caret2$resample$Accuracy)
cv.y_pred.2 = predict(model.caret2, newdata = test_set, type = 'raw')
table(cv.y_pred.2,test_set$income)
error=1-acc(cv.y_pred.2,test_set$income)
#error on the test set for the optimal model
error # 0.1795
roc.function(cv.y_pred.2,test_set$income)
roc.function(cv.y_pred.2,test_set$income) # 0.73
# HYPERPARAMETERS
control = trainControl(method="cv", number=10)
acc <- function(y.true, y.pred) { sum(y.pred==y.true)/length(y.true) }
library(ROCR)
roc.function<-function(y_pred,testY){
pred <- prediction(as.numeric(y_pred), as.numeric(testY))
perf.auc <- performance(pred, measure = "auc")
auc<-round(unlist(perf.auc@y.values),2)
perf <- performance(pred,"tpr","fpr")
plot(perf,main=paste("ROC and AUC=",auc),colorize=TRUE, lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2) #diagonal
}
dataset<-read.csv("income.csv", stringsAsFactors = T)
# install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$income, SplitRatio = 0.7)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# install.packages("rpart")
library(rpart)
library(rattle)
model= rpart(formula = income ~ ., data = training_set)
fancyRpartPlot(model)
y_pred = predict(model, newdata = training_set, type = 'class')
table(y_pred,training_set$income)
error=1-acc(y_pred,training_set$income)
#error on the training set for the optimal model
error # 0.1644
model.y_pred = predict(model, newdata = test_set, type = 'class')
table(model.y_pred,test_set$income)
error=1-acc(model.y_pred,test_set$income)
#error on the test set for the optimal model
error # 0.1773
#ROC curve for the optimal model
roc.function(model.y_pred, test_set$income) # 0.73
#ROC curve for the optimal model
roc.function(test_set$income, model.y_pred) # 0.73
#ROC curve for the optimal model
roc.function(model.y_pred, test_set$income) # 0.73
dataset<-read.csv("income.csv", stringsAsFactors = T)
# install.packages("caTools")
library(caTools)
set.seed(100)
split = sample.split(dataset$income, SplitRatio = 0.7)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# install.packages("rpart")
library(rpart)
library(rattle)
model= rpart(formula = income ~ ., data = training_set)
fancyRpartPlot(model)
y_pred = predict(model, newdata = training_set, type = 'class')
table(y_pred,training_set$income)
error=1-acc(y_pred,training_set$income)
#error on the training set for the optimal model
error # 0.1644
model.y_pred = predict(model, newdata = test_set, type = 'class')
table(model.y_pred,test_set$income)
error=1-acc(model.y_pred,test_set$income)
#error on the test set for the optimal model
error # 0.1773
#ROC curve for the optimal model
roc.function(model.y_pred, test_set$income) # 0.73
#CROSS VALIDATION
library(caret)
control = trainControl(method="cv", number=10)
model.caret = train(income ~ .,data=training_set, method="rpart", preProcess="scale", trControl=control)
plot(model.caret)
model.caret
model.caret$resample$Accuracy
sd(model.caret$resample$Accuracy)
cv.y_pred = predict(model.caret, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
#error on the test set for the optimal model
error # 0.1795
roc.function(cv.y_pred,test_set$income) # 0.73
control = trainControl(method="repeatedcv", number=10,repeats=3)
model.caret2 = train(income ~ .,data=training_set, method="rpart", preProcess="scale", trControl=control)
plot(model.caret2)
model.caret2
model.caret2$resample$Accuracy
sd(model.caret2$resample$Accuracy)
cv.y_pred.2 = predict(model.caret2, newdata = test_set, type = 'raw')
table(cv.y_pred.2,test_set$income)
error=1-acc(cv.y_pred.2,test_set$income)
#error on the test set for the optimal model
error # 0.1795
roc.function(cv.y_pred.2,test_set$income) # 0.73
# HYPERPARAMETERS
control = trainControl(method="cv", number=10)
parameter_grid = expand.grid(cp=c(0.01, 0.1, 0.2, 0.0005))
model.caret4 = train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control, tuneGrid = parameter_grid)
plot(model.caret4)
model.caret4
cv.y_pred = predict(model.caret4, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error # 0.1705
roc.function(cv.y_pred,test_set$income) # 0.73
model.randomSearch<- train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control,tuneLength = 15)
model.randomSearch
plot(model.randomSearch)
cv.y_pred = predict(model.randomSearch, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error # 0.1707
roc.function(cv.y_pred,test_set$income) # 0.73
# BOOTSTRAP
control = trainControl(method = "boot632", number = 3)
model.boot <- train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control)
model.boot
plot(model.boot)
cv.y_pred = predict(model.boot, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error # 0.1795
roc.function(cv.y_pred,test_set$income) # 0.73
library(ROCR)
acc <- function(y.true, y.pred) { sum(y.pred==y.true)/length(y.true) }
library(ROCR)
roc.function<-function(y_pred,testY){
pred <- prediction(as.numeric(y_pred), as.numeric(testY))
perf.auc <- performance(pred, measure = "auc")
auc<-round(unlist(perf.auc@y.values),2)
perf <- performance(pred,"tpr","fpr")
plot(perf,main=paste("ROC and AUC=",auc),colorize=TRUE, lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2) #diagonal
}
dataset<-read.csv("income.csv", stringsAsFactors = T)
# install.packages("caTools")
library(caTools)
set.seed(100)
split = sample.split(dataset$income, SplitRatio = 0.7)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# install.packages("rpart")
library(rpart)
library(rattle)
model= rpart(formula = income ~ ., data = training_set)
fancyRpartPlot(model)
y_pred = predict(model, newdata = training_set, type = 'class')
table(y_pred,training_set$income)
error=1-acc(y_pred,training_set$income)
#error on the training set for the optimal model
error # 0.1644
#error on the training set for the optimal model
error # 0.1680
model.y_pred = predict(model, newdata = test_set, type = 'class')
table(model.y_pred,test_set$income)
error=1-acc(model.y_pred,test_set$income)
#error on the test set for the optimal model
error # 0.1773
#error on the test set for the optimal model
error # 0.1692
#ROC curve for the optimal model
roc.function(model.y_pred, test_set$income) # 0.73
#ROC curve for the optimal model
roc.function(model.y_pred, test_set$income) # 0.74
#CROSS VALIDATION
library(caret)
control = trainControl(method="cv", number=10)
model.caret = train(income ~ .,data=training_set, method="rpart", preProcess="scale", trControl=control)
plot(model.caret)
model.caret
model.caret$resample$Accuracy
sd(model.caret$resample$Accuracy)
cv.y_pred = predict(model.caret, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
#error on the test set for the optimal model
error # 0.1795
#error on the test set for the optimal model
error # 0.1764
roc.function(cv.y_pred,test_set$income) # 0.73
roc.function(cv.y_pred,test_set$income) # 0.7
control = trainControl(method="repeatedcv", number=10,repeats=3)
model.caret2 = train(income ~ .,data=training_set, method="rpart", preProcess="scale", trControl=control)
plot(model.caret2)
model.caret2
model.caret2$resample$Accuracy
sd(model.caret2$resample$Accuracy)
cv.y_pred.2 = predict(model.caret2, newdata = test_set, type = 'raw')
table(cv.y_pred.2,test_set$income)
error=1-acc(cv.y_pred.2,test_set$income)
#error on the test set for the optimal model
error # 0.1795
#error on the test set for the optimal model
error # 0.1765
roc.function(cv.y_pred.2,test_set$income) # 0.73
roc.function(cv.y_pred.2,test_set$income) # 0.7
# HYPERPARAMETERS
control = trainControl(method="cv", number=10)
parameter_grid = expand.grid(cp=c(0.01, 0.1, 0.2, 0.0005))
model.caret4 = train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control, tuneGrid = parameter_grid)
plot(model.caret4)
model.caret4
cv.y_pred = predict(model.caret4, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error # 0.1705
roc.function(cv.y_pred,test_set$income) # 0.73
roc.function(cv.y_pred,test_set$income) # 0.75
model.randomSearch<- train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control,tuneLength = 15)
model.randomSearch
plot(model.randomSearch)
cv.y_pred = predict(model.randomSearch, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error # 0.1707
roc.function(cv.y_pred,test_set$income) # 0.73
error # 0.1648
roc.function(cv.y_pred,test_set$income) # 0.74
# BOOTSTRAP
control = trainControl(method = "boot632", number = 3)
model.boot <- train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control)
model.boot
plot(model.boot)
cv.y_pred = predict(model.boot, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error # 0.1795
roc.function(cv.y_pred,test_set$income) # 0.73
# Rules of Grid Search Optimization model
asRules(model.randomSearch)
rpart.rules(model.boot$finalModel, model.boot$terms)
# Rules of Grid Search Optimization model
library(rpart.plot)
asRules(model.randomSearch)
rpart.rules(model.boot$finalModel, model.boot$terms)
View(model.randomSearch)
View(model)
asRules(model.randomSearch$finalModel)
fancyRpartPlot(model.randomSearch$finalModel)
# Exercise A8 -------------------------------------------------------------
# Pablo Olivares Martinez -------------------------------------------------
library(rpart)
library(rattle)
library(caTools)
library(ROCR)
library(caret)
# Declaration of the ACC and ROC functions
acc <- function(y.true, y.pred) { sum(y.pred==y.true)/length(y.true) }
roc.function<-function(y_pred,testY){
pred <- prediction(as.numeric(y_pred), as.numeric(testY))
perf.auc <- performance(pred, measure = "auc")
auc<-round(unlist(perf.auc@y.values),2)
perf <- performance(pred,"tpr","fpr")
plot(perf,main=paste("ROC and AUC=",auc),colorize=TRUE, lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2) #diagonal
}
dataset<-read.csv("income.csv", stringsAsFactors = T)
# Training the default model
set.seed(100)
split = sample.split(dataset$income, SplitRatio = 0.7)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
model= rpart(formula = income ~ ., data = training_set)
fancyRpartPlot(model)
y_pred = predict(model, newdata = training_set, type = 'class')
table(y_pred,training_set$income)
error=1-acc(y_pred,training_set$income)
# error on the training set for the optimal model
error # 0.1680
# Default model
model.y_pred = predict(model, newdata = test_set, type = 'class')
table(model.y_pred,test_set$income)
error=1-acc(model.y_pred,test_set$income)
# error on the test set for the optimal model
error # 0.1692
# ROC curve for the optimal model
roc.function(model.y_pred, test_set$income) # 0.74
# Now we will get various optimizations and compare them with the original model:
# CROSS VALIDATION
# K-Fold Cross Validation
control = trainControl(method="cv", number=10)
model.caret = train(income ~ .,data=training_set, method="rpart", preProcess="scale", trControl=control)
plot(model.caret)
model.caret
model.caret$resample$Accuracy
sd(model.caret$resample$Accuracy)
cv.y_pred = predict(model.caret, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
#error on the test set for the optimal model
error # 0.1764
roc.function(cv.y_pred,test_set$income) # 0.7
# Now with repetition
control = trainControl(method="repeatedcv", number=10,repeats=3)
model.caret2 = train(income ~ .,data=training_set, method="rpart", preProcess="scale", trControl=control)
plot(model.caret2)
model.caret2
model.caret2$resample$Accuracy
sd(model.caret2$resample$Accuracy)
cv.y_pred.2 = predict(model.caret2, newdata = test_set, type = 'raw')
table(cv.y_pred.2,test_set$income)
error=1-acc(cv.y_pred.2,test_set$income)
# error on the test set for the optimal model
error # 0.1765
roc.function(cv.y_pred.2,test_set$income) # 0.7
# HYPERPARAMETER OPTIMIZATION
# Grid Search Optimization
control = trainControl(method="cv", number=10)
parameter_grid = expand.grid(cp=c(0.01, 0.1, 0.2, 0.0005))
model.caret4 = train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control, tuneGrid = parameter_grid)
plot(model.caret4)
model.caret4
cv.y_pred = predict(model.caret4, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error # 0.1646
roc.function(cv.y_pred,test_set$income) # 0.75
# Random Search Optimization
model.randomSearch<- train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control,tuneLength = 15)
model.randomSearch
plot(model.randomSearch)
cv.y_pred = predict(model.randomSearch, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error # 0.1648
roc.function(cv.y_pred,test_set$income) # 0.74
# BOOTSTRAP
control = trainControl(method = "boot632", number = 3)
model.boot <- train(income ~ .,data=training_set, method="rpart", preProcess="scale",
trControl=control)
model.boot
plot(model.boot)
cv.y_pred = predict(model.boot, newdata = test_set, type = 'raw')
table(cv.y_pred,test_set$income)
error=1-acc(cv.y_pred,test_set$income)
error # 0.1765
roc.function(cv.y_pred,test_set$income) # 0.7
# As we can see after evaluating the error rate and the AUC of the ROC, the best optimization
# is the one obtained by modifying the hyperparameters, in particular using the Grid Search Optimization,
# which finds the hyperparameters of the model that get the best result from the combination of some
# given parameters. The obtained AUC (Area Under Curve) 0.75 > 0.74, the default obtained.  This means
# that our model returns fewer false positives and higher positive rate. Moreover, the
# error rate is smaller (0.1692 > 0.1646). After this, let us obtain the rules of this model.
# Rules of Grid Search Optimization model
asRules(model.randomSearch$finalModel)
# Visualization of the tree
fancyRpartPlot(model.randomSearch$finalModel)
# Here I get the rules of the optimized model. It is a bit easier to compare the rules of this model
# looking at the given tree. As we can see, with the optimization we obtain a finer model, with more rules
# and cases than the original. However, it avoids the disadvantages of getting a quite fine model, where
# we would probably have overfitting.
